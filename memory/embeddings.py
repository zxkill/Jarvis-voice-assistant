"""Локальное преобразование текста в векторные представления.

Модуль содержит простую реализацию эмбеддингов без внешних
зависимостей. В качестве примера используется хеширование слов в
фиксированный размерность вектора. Такой подход позволяет получить
стабильные и воспроизводимые вектора, пригодные для поиска схожих
фрагментов текста в рамках офлайн‑системы.
"""

from __future__ import annotations

# Стандартные библиотеки
import hashlib
import logging
from typing import List

import numpy as np

# Логгер для отслеживания вычислений эмбеддингов
logger = logging.getLogger(__name__)

# Размерность вектора по умолчанию. Увеличенное значение снижает
# вероятность коллизий при хешировании и повышает качество поиска.
# Большая размерность практически исключает коллизии между словами
# даже в небольших корпусах текста.
VECTOR_SIZE = 4096


def get_embedding(text: str, *, size: int = VECTOR_SIZE) -> List[float]:
    """Преобразовать *text* в вектор фиксированной длины.

    Вектор строится путём хеширования каждого слова и инкремента
    соответствующего элемента. Результат нормализуется по длине, что
    позволяет корректно сравнивать различные предложения по косинусному
    сходству.
    """

    # Инициализируем нулевой вектор требуемой размерности
    vec = np.zeros(size, dtype=float)
    logger.debug("Вычисление эмбеддинга для текста: %r", text)

    for word in text.lower().split():
        # Получаем целочисленный хеш слова
        digest = hashlib.sha256(word.encode("utf-8")).digest()
        # Преобразуем первые четыре байта в индекс
        idx = int.from_bytes(digest[:4], "little") % size
        vec[idx] += 1.0

    # Нормализуем вектор, чтобы длина была равна единице
    norm = np.linalg.norm(vec)
    if norm > 0:
        vec /= norm

    logger.debug("Эмбеддинг: %s", vec)
    # Возвращаем обычный список для последующей сериализации в JSON
    return vec.tolist()
